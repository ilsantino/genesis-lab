"""
Training Page - Model training and evaluation interface.

Provides tools for training intent classifiers with different configurations,
running cross-validation, and tracking experiments.
"""

import streamlit as st
import json
from pathlib import Path
from datetime import datetime

# Import components
from ui.components.styles import get_divider
from ui.components.cards import info_banner, stat_card, metric_card, page_header, empty_state
from ui.components.charts import quality_gauge, metrics_radar_chart

# Try to import backend
BACKEND_AVAILABLE = False
REGISTRY_AVAILABLE = False

try:
    from src.training import (
        Trainer,
        IntentClassifier,
        get_preset,
        list_presets,
        PRESETS,
        CVResult,
        ExperimentResult,
    )
    from src.utils.visualization import load_conversations
    BACKEND_AVAILABLE = True
except ImportError:
    pass

try:
    from src.registry import DatasetRegistry
    REGISTRY_AVAILABLE = True
except ImportError:
    pass


def render_training_page():
    """Render the training page."""
    page_header(
        icon="üéì",
        title="Model Training",
        subtitle="Train and evaluate intent classification models."
    )
    
    # Check backend availability
    if not BACKEND_AVAILABLE:
        info_banner(
            "Demo Mode - Training backend not connected. Features are simulated.",
            type="warning",
            icon="üîß"
        )
    
    # Training tabs
    tab_train, tab_evaluate, tab_history, tab_learn = st.tabs([
        "üöÄ Train Model",
        "üìä Evaluate",
        "üìú History",
        "üìñ Learn"
    ])
    
    with tab_train:
        render_train_tab()
    
    with tab_evaluate:
        render_evaluate_tab()
    
    with tab_history:
        render_history_tab()
    
    with tab_learn:
        render_learn_tab()


def render_train_tab():
    """Render the training configuration and execution tab."""
    
    # Main layout
    col_config, col_results = st.columns([1, 1])
    
    with col_config:
        st.markdown('<p class="section-header">Training Configuration</p>', unsafe_allow_html=True)
        
        # Dataset selection help
        with st.expander("‚ùì What data should I use?", expanded=False):
            st.markdown("""
            **Use synthetic data generated by Genesis Lab.**
            
            - Go to the **Generate** page first to create conversations
            - Or upload a JSON file with the same conversation format
            - Minimum recommended: **100 conversations** for testing
            - For production: **1,000+ conversations** recommended
            
            The data should contain conversations with `intent`, `turns`, and other Banking77 fields.
            """)
        
        # Dataset selection
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("**Select Dataset**")
        
        # Option to upload or select existing
        data_source = st.radio(
            "Data Source",
            options=["existing", "upload"],
            format_func=lambda x: "Select Existing" if x == "existing" else "Upload File",
            horizontal=True,
            label_visibility="collapsed"
        )
        
        dataset_path = None
        conversations = []
        
        if data_source == "existing":
            # List available datasets
            data_dir = Path("data/synthetic")
            json_files = []
            if data_dir.exists():
                json_files = sorted(
                    [f.name for f in data_dir.glob("*.json") if not f.name.endswith(".report.json")],
                    reverse=True
                )
            
            selected_file = st.selectbox(
                "Choose a dataset",
                options=[""] + json_files,
                format_func=lambda x: "Select a file..." if x == "" else x,
                label_visibility="collapsed"
            )
            
            if selected_file:
                dataset_path = str(data_dir / selected_file)
                if BACKEND_AVAILABLE:
                    conversations = load_conversations(dataset_path)
                else:
                    try:
                        with open(dataset_path) as f:
                            conversations = json.load(f)
                    except Exception:
                        pass
        else:
            uploaded_file = st.file_uploader(
                "Upload JSON file",
                type=["json"],
                label_visibility="collapsed"
            )
            if uploaded_file:
                try:
                    conversations = json.load(uploaded_file)
                    dataset_path = uploaded_file.name
                except Exception as e:
                    st.error(f"Error loading file: {e}")
        
        if conversations:
            st.success(f"Loaded {len(conversations)} conversations")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Preset selection
        st.markdown('<p class="section-header" style="margin-top: 1.5rem;">Quick Presets</p>', unsafe_allow_html=True)
        
        # Preset help
        with st.expander("‚ùì Which preset should I choose?", expanded=False):
            st.markdown("""
            **Start with Balanced if you're unsure!**
            
            | Preset | Model | Use When |
            |--------|-------|----------|
            | ‚ö° **Fast** | Logistic Regression | Quick tests, debugging, iteration |
            | ‚öñÔ∏è **Balanced** | Random Forest | Most situations, good all-around |
            | üéØ **Best** | XGBoost | Final production model, need max accuracy |
            
            The preset auto-configures the model type and settings below.
            """)
        
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        
        preset_col1, preset_col2, preset_col3 = st.columns(3)
        
        # Initialize session state for preset
        if "selected_preset" not in st.session_state:
            st.session_state.selected_preset = "balanced"
        
        with preset_col1:
            if st.button("‚ö° Fast", use_container_width=True, 
                        type="primary" if st.session_state.selected_preset == "fast" else "secondary"):
                st.session_state.selected_preset = "fast"
                st.rerun()
            st.caption("LogisticRegression\nQuick training")
        
        with preset_col2:
            if st.button("‚öñÔ∏è Balanced", use_container_width=True,
                        type="primary" if st.session_state.selected_preset == "balanced" else "secondary"):
                st.session_state.selected_preset = "balanced"
                st.rerun()
            st.caption("RandomForest\nGood accuracy")
        
        with preset_col3:
            if st.button("üéØ Best", use_container_width=True,
                        type="primary" if st.session_state.selected_preset == "best" else "secondary"):
                st.session_state.selected_preset = "best"
                st.rerun()
            st.caption("XGBoost\nHighest accuracy")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Model configuration
        st.markdown('<p class="section-header" style="margin-top: 1.5rem;">Model Configuration</p>', unsafe_allow_html=True)
        
        # Model config help
        with st.expander("‚ùì What do these settings mean?", expanded=False):
            st.markdown("""
            **Model Type:** The algorithm used for classification.
            - *Logistic Regression* - Fast, simple, good baseline
            - *Random Forest* - Ensemble of decision trees, more robust
            - *XGBoost* - Gradient boosting, highest accuracy
            
            **Cross-Validation:** Trains/tests multiple times for reliable results.
            - Recommended for final evaluation
            - Takes longer but gives confidence intervals (¬±)
            
            **Advanced Options:**
            - *Max Features* - How many words to consider (higher = more detail, slower)
            - *N-gram Range* - Word combinations to use (1,2 = single words + pairs)
            - *Test Split* - What % of data to save for testing (default 20%)
            """)
        
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        
        # Model type
        model_type = st.selectbox(
            "Model Type",
            options=["logistic_regression", "random_forest", "xgboost"],
            format_func=lambda x: {
                "logistic_regression": "Logistic Regression",
                "random_forest": "Random Forest",
                "xgboost": "XGBoost"
            }.get(x, x),
            index=["logistic_regression", "random_forest", "xgboost"].index(
                _get_preset_model_type(st.session_state.selected_preset)
            )
        )
        
        # Cross-validation
        use_cv = st.checkbox("Enable Cross-Validation", value=False)
        cv_folds = 5
        if use_cv:
            cv_folds = st.slider("Number of Folds", min_value=3, max_value=10, value=5)
        
        # Advanced options
        with st.expander("Advanced Options"):
            st.caption("üí° Default values work well for most cases")
            
            max_features = st.slider(
                "Max Features (TF-IDF)",
                min_value=1000,
                max_value=10000,
                value=5000,
                step=1000,
                help="Number of unique words/phrases to consider. Higher = more detail but slower."
            )
            
            st.caption("N-gram range: (1,2) means use single words AND word pairs")
            ngram_min, ngram_max = st.columns(2)
            with ngram_min:
                ngram_lower = st.number_input("N-gram Min", min_value=1, max_value=3, value=1, help="Minimum word group size")
            with ngram_max:
                ngram_upper = st.number_input("N-gram Max", min_value=1, max_value=3, value=2, help="Maximum word group size")
            
            test_size = st.slider(
                "Test Split",
                min_value=0.1,
                max_value=0.4,
                value=0.2,
                step=0.05,
                format="%.2f",
                help="Fraction of data held out for testing. 0.2 = 20% test, 80% train."
            )
            
            save_model = st.checkbox("Save Trained Model", value=True, help="Save to models/trained/ for later use")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Train button
        st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)
        
        train_disabled = not conversations
        train_clicked = st.button(
            "üöÄ Start Training",
            use_container_width=True,
            type="primary",
            disabled=train_disabled
        )
        
        if train_disabled:
            info_banner("Select a dataset to enable training.", type="info")
        
        if train_clicked and conversations:
            run_training(
                conversations=conversations,
                model_type=model_type,
                use_cv=use_cv,
                cv_folds=cv_folds,
                max_features=max_features,
                ngram_range=(int(ngram_lower), int(ngram_upper)),
                test_size=test_size,
                save_model=save_model,
                dataset_path=dataset_path
            )
    
    with col_results:
        st.markdown('<p class="section-header">Training Results</p>', unsafe_allow_html=True)
        
        if "last_training_results" in st.session_state:
            show_training_results(st.session_state.last_training_results)
        else:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            empty_state(
                title="No Training Results Yet",
                message="Configure and run training to see results here.",
                icon="üéì"
            )
            st.markdown('</div>', unsafe_allow_html=True)


def render_evaluate_tab():
    """Render the model evaluation tab."""
    st.markdown('<p class="section-header">Model Evaluation</p>', unsafe_allow_html=True)
    
    col_model, col_data = st.columns([1, 1])
    
    with col_model:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("**Select Model**")
        
        # List available models
        models_dir = Path("models/trained")
        model_files = []
        if models_dir.exists():
            model_files = sorted([f.name for f in models_dir.glob("*.pkl")], reverse=True)
        
        if not model_files:
            info_banner("No trained models found. Train a model first.", type="info")
        else:
            selected_model = st.selectbox(
                "Choose a model",
                options=model_files,
                label_visibility="collapsed"
            )
        st.markdown('</div>', unsafe_allow_html=True)
    
    with col_data:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("**Select Test Data**")
        
        # List available datasets
        data_dir = Path("data/synthetic")
        json_files = []
        if data_dir.exists():
            json_files = sorted(
                [f.name for f in data_dir.glob("*.json") if not f.name.endswith(".report.json")],
                reverse=True
            )
        
        selected_test_file = st.selectbox(
            "Choose test dataset",
            options=[""] + json_files,
            format_func=lambda x: "Select a file..." if x == "" else x,
            label_visibility="collapsed",
            key="eval_dataset"
        )
        st.markdown('</div>', unsafe_allow_html=True)
    
    get_divider()
    
    # Evaluate button
    can_evaluate = model_files and selected_test_file
    
    if st.button("üìä Evaluate Model", use_container_width=True, type="primary", disabled=not can_evaluate):
        if BACKEND_AVAILABLE:
            run_evaluation(
                model_path=str(models_dir / selected_model),
                test_data_path=str(data_dir / selected_test_file)
            )
        else:
            # Demo evaluation
            st.session_state.last_evaluation_results = {
                "accuracy": 0.72,
                "f1_score": 0.68,
                "precision": 0.70,
                "recall": 0.66,
                "demo": True
            }
            st.rerun()
    
    # Show evaluation results
    if "last_evaluation_results" in st.session_state:
        show_evaluation_results(st.session_state.last_evaluation_results)


def render_history_tab():
    """Render the training history tab."""
    st.markdown('<p class="section-header">Training History</p>', unsafe_allow_html=True)
    
    if not REGISTRY_AVAILABLE:
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        info_banner("Registry not available. Training history requires the registry module.", type="warning")
        st.markdown('</div>', unsafe_allow_html=True)
        return
    
    try:
        registry = DatasetRegistry()
        
        # Get all datasets with training runs
        datasets = registry.list_datasets()
        
        if not datasets:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            empty_state(
                title="No Training History",
                message="Train models on registered datasets to see history here.",
                icon="üìú"
            )
            st.markdown('</div>', unsafe_allow_html=True)
            return
        
        # Collect all training runs
        all_runs = []
        for dataset in datasets:
            history = registry.get_training_history(dataset["id"])
            for run in history:
                run["dataset_name"] = dataset.get("name", dataset["id"])
                all_runs.append(run)
        
        if not all_runs:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            empty_state(
                title="No Training Runs Found",
                message="Training runs will appear here after you train models.",
                icon="üìú"
            )
            st.markdown('</div>', unsafe_allow_html=True)
            return
        
        # Sort by date
        all_runs.sort(key=lambda x: x.get("trained_at", ""), reverse=True)
        
        # Summary stats
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            stat_card(
                value=str(len(all_runs)),
                label="Total Runs",
                icon="üîÑ"
            )
        
        with col2:
            # Best accuracy
            best_acc = max(
                (r.get("results", {}).get("accuracy", 0) for r in all_runs),
                default=0
            )
            stat_card(
                value=f"{best_acc*100:.1f}%",
                label="Best Accuracy",
                icon="üèÜ"
            )
        
        with col3:
            # Model types used
            model_types = set(r.get("model_type", "unknown") for r in all_runs)
            stat_card(
                value=str(len(model_types)),
                label="Model Types",
                icon="ü§ñ"
            )
        
        with col4:
            # Datasets trained
            dataset_names = set(r.get("dataset_name", "unknown") for r in all_runs)
            stat_card(
                value=str(len(dataset_names)),
                label="Datasets",
                icon="üìÅ"
            )
        
        get_divider()
        
        # Training runs table
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("**Recent Training Runs**")
        
        for run in all_runs[:10]:  # Show last 10
            results = run.get("results", {})
            accuracy = results.get("accuracy", 0)
            f1 = results.get("f1_score", 0)
            
            col_info, col_metrics = st.columns([2, 1])
            
            with col_info:
                st.markdown(f"""
                **{run.get('model_type', 'Unknown').replace('_', ' ').title()}** on `{run.get('dataset_name', 'Unknown')}`  
                <span style="color: #718096; font-size: 0.85rem;">{run.get('trained_at', 'Unknown date')}</span>
                """, unsafe_allow_html=True)
            
            with col_metrics:
                status = "success" if accuracy >= 0.7 else "warning" if accuracy >= 0.5 else "error"
                st.markdown(f"""
                <div style="text-align: right;">
                    <span style="color: {'#10b981' if status == 'success' else '#f59e0b' if status == 'warning' else '#ef4444'}; font-weight: 600;">
                        {accuracy*100:.1f}% acc
                    </span>
                    <span style="color: #718096;"> | </span>
                    <span style="color: #a0aec0;">{f1*100:.1f}% F1</span>
                </div>
                """, unsafe_allow_html=True)
            
            st.markdown("---")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
    except Exception as e:
        st.error(f"Error loading training history: {e}")


def render_learn_tab():
    """Render the educational Learn tab with training concepts explained."""
    
    # What is Training - The most important concept
    st.markdown('<p class="section-header">What is Training in Genesis Lab?</p>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="glass-card" style="border-left: 4px solid #f59e0b;">
        <h4 style="color: #f59e0b; margin-bottom: 1rem;">‚ö†Ô∏è Important: You do NOT train the AI that generates data!</h4>
        <p style="color: #a0aec0; line-height: 1.8; font-size: 1rem;">
            This is the most common misconception. <strong style="color: white;">Claude (the AI)</strong> is already trained by Anthropic. 
            You cannot and do not need to train it. Instead, you train a <strong style="color: white;">separate classifier model</strong> 
            that learns from the synthetic data Claude generates.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Visual Pipeline Explanation
    st.markdown('<p class="section-header" style="margin-top: 2rem;">The Complete Pipeline (Step by Step)</p>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="glass-card" style="text-align: center; min-height: 280px; border-top: 3px solid #667eea;">
            <div style="font-size: 3rem; margin-bottom: 1rem;">1Ô∏è‚É£</div>
            <h4 style="color: #667eea; margin-bottom: 0.5rem;">GENERATE</h4>
            <p style="color: white; font-weight: 600; margin-bottom: 0.5rem;">Claude AI creates data</p>
            <p style="color: #a0aec0; font-size: 0.85rem; line-height: 1.6;">
                You configure settings (language, samples). Claude generates realistic customer 
                service conversations following the Banking77 taxonomy.
            </p>
            <p style="color: #718096; font-size: 0.8rem; margin-top: 1rem;">
                <em>Claude = The author writing examples</em>
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="glass-card" style="text-align: center; min-height: 280px; border-top: 3px solid #10b981;">
            <div style="font-size: 3rem; margin-bottom: 1rem;">2Ô∏è‚É£</div>
            <h4 style="color: #10b981; margin-bottom: 0.5rem;">VALIDATE</h4>
            <p style="color: white; font-weight: 600; margin-bottom: 0.5rem;">Check data quality</p>
            <p style="color: #a0aec0; font-size: 0.85rem; line-height: 1.6;">
                The validation module checks completeness, consistency, realism, and diversity. 
                It also detects bias in sentiment, language, and intent distribution.
            </p>
            <p style="color: #718096; font-size: 0.8rem; margin-top: 1rem;">
                <em>Quality Control = The editor reviewing work</em>
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="glass-card" style="text-align: center; min-height: 280px; border-top: 3px solid #ec4899;">
            <div style="font-size: 3rem; margin-bottom: 1rem;">3Ô∏è‚É£</div>
            <h4 style="color: #ec4899; margin-bottom: 0.5rem;">TRAIN</h4>
            <p style="color: white; font-weight: 600; margin-bottom: 0.5rem;">Teach a classifier</p>
            <p style="color: #a0aec0; font-size: 0.85rem; line-height: 1.6;">
                A machine learning model learns to classify intents from the synthetic data. 
                This trained model can then predict intents for new, unseen messages.
            </p>
            <p style="color: #718096; font-size: 0.8rem; margin-top: 1rem;">
                <em>Classifier = A student learning from examples</em>
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    # Flow diagram
    st.markdown("""
    <div class="glass-card" style="text-align: center; padding: 1.5rem;">
        <p style="color: #a0aec0; font-size: 1.1rem; letter-spacing: 0.5px;">
            <span style="color: #667eea;">Your Settings</span> 
            ‚Üí <span style="color: white;">Claude AI</span> 
            ‚Üí <span style="color: #667eea;">Synthetic Data</span> 
            ‚Üí <span style="color: #10b981;">Quality Check</span> 
            ‚Üí <span style="color: #ec4899;">Train Classifier</span> 
            ‚Üí <span style="color: white;">Trained Model</span> 
            ‚Üí <span style="color: #f59e0b;">Predict Intents!</span>
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    get_divider()
    
    # Model Comparison
    st.markdown('<p class="section-header">Available Models Comparison</p>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="glass-card">
        <p style="color: #a0aec0; margin-bottom: 1.5rem;">
            Genesis Lab supports three classification algorithms. Each has different trade-offs between 
            speed and accuracy. All use <strong style="color: white;">TF-IDF</strong> (Term Frequency-Inverse Document Frequency) 
            to convert text into numbers that the model can understand.
        </p>
        <table style="width: 100%; border-collapse: collapse; color: #a0aec0;">
            <thead>
                <tr style="border-bottom: 2px solid #4a5568;">
                    <th style="padding: 1rem; text-align: left; color: white;">Model</th>
                    <th style="padding: 1rem; text-align: center; color: white;">Speed</th>
                    <th style="padding: 1rem; text-align: center; color: white;">Accuracy</th>
                    <th style="padding: 1rem; text-align: left; color: white;">Best For</th>
                </tr>
            </thead>
            <tbody>
                <tr style="border-bottom: 1px solid #2d3748;">
                    <td style="padding: 1rem;"><strong style="color: #667eea;">Logistic Regression</strong></td>
                    <td style="padding: 1rem; text-align: center;"><span style="color: #10b981;">‚ö° Very Fast</span><br><small>(seconds)</small></td>
                    <td style="padding: 1rem; text-align: center;"><span style="color: #f59e0b;">Good</span><br><small>baseline</small></td>
                    <td style="padding: 1rem;">Quick experiments, iteration, debugging</td>
                </tr>
                <tr style="border-bottom: 1px solid #2d3748;">
                    <td style="padding: 1rem;"><strong style="color: #10b981;">Random Forest</strong></td>
                    <td style="padding: 1rem; text-align: center;"><span style="color: #f59e0b;">Medium</span><br><small>(10-30 sec)</small></td>
                    <td style="padding: 1rem; text-align: center;"><span style="color: #10b981;">Better</span><br><small>reliable</small></td>
                    <td style="padding: 1rem;">Production with limited data, feature importance</td>
                </tr>
                <tr>
                    <td style="padding: 1rem;"><strong style="color: #ec4899;">XGBoost</strong></td>
                    <td style="padding: 1rem; text-align: center;"><span style="color: #ef4444;">Slower</span><br><small>(30-60 sec)</small></td>
                    <td style="padding: 1rem; text-align: center;"><span style="color: #10b981;">Best</span><br><small>highest</small></td>
                    <td style="padding: 1rem;">Final production model, competitions</td>
                </tr>
            </tbody>
        </table>
    </div>
    """, unsafe_allow_html=True)
    
    get_divider()
    
    # How to Choose
    st.markdown('<p class="section-header">How to Choose a Model</p>', unsafe_allow_html=True)
    
    col_choose1, col_choose2 = st.columns(2)
    
    with col_choose1:
        st.markdown("""
        <div class="glass-card">
            <h4 style="color: #667eea; margin-bottom: 1rem;">üöÄ Use Logistic Regression when:</h4>
            <ul style="color: #a0aec0; line-height: 2;">
                <li>You're just testing if training works</li>
                <li>You need results in seconds, not minutes</li>
                <li>You're iterating quickly on data generation</li>
                <li>You want an interpretable baseline</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="glass-card" style="margin-top: 1rem;">
            <h4 style="color: #10b981; margin-bottom: 1rem;">üå≤ Use Random Forest when:</h4>
            <ul style="color: #a0aec0; line-height: 2;">
                <li>You have less than 500 samples</li>
                <li>You want to see which features matter most</li>
                <li>You need a balance of speed and accuracy</li>
                <li>You want robust results with less tuning</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col_choose2:
        st.markdown("""
        <div class="glass-card">
            <h4 style="color: #ec4899; margin-bottom: 1rem;">üéØ Use XGBoost when:</h4>
            <ul style="color: #a0aec0; line-height: 2;">
                <li>You have 1000+ samples</li>
                <li>You need the absolute best accuracy</li>
                <li>You're building a production model</li>
                <li>Training time is not a concern</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="glass-card" style="margin-top: 1rem; border-left: 3px solid #f59e0b;">
            <h4 style="color: #f59e0b; margin-bottom: 1rem;">üí° Pro Tip: Use Presets!</h4>
            <p style="color: #a0aec0; line-height: 1.8;">
                The <strong>Fast</strong>, <strong>Balanced</strong>, and <strong>Best</strong> presets 
                automatically configure the right model and settings for each use case. 
                Start with <strong>Balanced</strong> if you're unsure.
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    get_divider()
    
    # Understanding Results
    st.markdown('<p class="section-header">Understanding Your Results</p>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="glass-card">
        <p style="color: #a0aec0; margin-bottom: 1.5rem;">
            After training, you'll see several metrics. Here's what they mean in plain English:
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    col_met1, col_met2 = st.columns(2)
    
    with col_met1:
        st.markdown("""
        <div class="glass-card">
            <h4 style="color: #10b981; margin-bottom: 0.5rem;">üéØ Accuracy</h4>
            <p style="color: #a0aec0; line-height: 1.8; margin-bottom: 1rem;">
                <strong style="color: white;">What % of predictions were correct?</strong><br>
                If accuracy is 70%, the model correctly identified the intent 70 out of 100 times.
            </p>
            <p style="color: #718096; font-size: 0.85rem;">
                <strong>Good:</strong> 70%+ | <strong>Great:</strong> 85%+ | <strong>Excellent:</strong> 95%+
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="glass-card" style="margin-top: 1rem;">
            <h4 style="color: #667eea; margin-bottom: 0.5rem;">‚úì Precision</h4>
            <p style="color: #a0aec0; line-height: 1.8; margin-bottom: 1rem;">
                <strong style="color: white;">When the model predicts an intent, how often is it right?</strong><br>
                High precision = fewer false alarms. Important when mistakes are costly.
            </p>
            <p style="color: #718096; font-size: 0.85rem;">
                Example: 80% precision for "lost_card" means 8/10 predictions were actually lost card issues.
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    with col_met2:
        st.markdown("""
        <div class="glass-card">
            <h4 style="color: #ec4899; margin-bottom: 0.5rem;">üìä F1 Score</h4>
            <p style="color: #a0aec0; line-height: 1.8; margin-bottom: 1rem;">
                <strong style="color: white;">The balance between precision and recall.</strong><br>
                A single number that combines both metrics. Higher is better. Use this to compare models.
            </p>
            <p style="color: #718096; font-size: 0.85rem;">
                <strong>Good:</strong> 65%+ | <strong>Great:</strong> 80%+ | <strong>Excellent:</strong> 90%+
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="glass-card" style="margin-top: 1rem;">
            <h4 style="color: #f59e0b; margin-bottom: 0.5rem;">üîç Recall</h4>
            <p style="color: #a0aec0; line-height: 1.8; margin-bottom: 1rem;">
                <strong style="color: white;">Of all actual instances, how many did the model find?</strong><br>
                High recall = fewer missed cases. Important when you can't afford to miss anything.
            </p>
            <p style="color: #718096; font-size: 0.85rem;">
                Example: 75% recall for "fraud" means we caught 75 out of 100 actual fraud cases.
            </p>
        </div>
        """, unsafe_allow_html=True)
    
    get_divider()
    
    # Glossary
    st.markdown('<p class="section-header">Key Terms Glossary</p>', unsafe_allow_html=True)
    
    with st.expander("üìö TF-IDF (Term Frequency-Inverse Document Frequency)", expanded=False):
        st.markdown("""
        **What it does:** Converts text into numbers that machine learning models can understand.
        
        **How it works:** 
        - Counts how often each word appears in a message (Term Frequency)
        - Reduces the weight of common words like "the", "is", "a" (Inverse Document Frequency)
        - Result: Important/unique words get higher scores
        
        **Example:** In "My card was stolen", "card" and "stolen" get high scores, "my" and "was" get low scores.
        """)
    
    with st.expander("üî¢ N-grams", expanded=False):
        st.markdown("""
        **What it does:** Captures word combinations, not just individual words.
        
        **Types:**
        - **Unigrams (1):** Single words ‚Üí "card", "stolen", "help"
        - **Bigrams (2):** Two-word pairs ‚Üí "card stolen", "need help"
        - **Trigrams (3):** Three-word groups ‚Üí "card was stolen"
        
        **Why it matters:** "not working" has different meaning than "not" and "working" separately.
        
        **Setting:** N-gram range (1,2) means use both unigrams AND bigrams.
        """)
    
    with st.expander("üîÑ Cross-Validation", expanded=False):
        st.markdown("""
        **What it does:** Tests the model multiple times on different portions of your data.
        
        **How 5-fold CV works:**
        1. Split data into 5 equal parts
        2. Train on 4 parts, test on 1 part
        3. Repeat 5 times, each time using a different test part
        4. Average the 5 results
        
        **Why it matters:** Gives a more reliable accuracy estimate than a single train/test split.
        
        **When to use:** When you want to be confident in your results, or when you have limited data.
        """)
    
    with st.expander("üéØ Intent", expanded=False):
        st.markdown("""
        **What it is:** The purpose or goal behind a customer's message.
        
        **Banking77 Examples:**
        - "Where is my card?" ‚Üí Intent: `card_arrival`
        - "I lost my card" ‚Üí Intent: `lost_or_stolen_card`
        - "How do I add money?" ‚Üí Intent: `top_up_by_card`
        
        **Why 77 intents:** The Banking77 benchmark has 77 distinct customer service intents, making it a challenging classification task.
        """)
    
    with st.expander("üìä Train/Test Split", expanded=False):
        st.markdown("""
        **What it does:** Divides your data into two parts.
        
        **Training set (80%):** The model learns from this data.
        
        **Test set (20%):** Used to evaluate how well the model performs on unseen data.
        
        **Why split:** If you test on the same data you trained on, you'll get artificially high scores (overfitting).
        
        **Setting:** Test size 0.2 = 20% for testing, 80% for training.
        """)
    
    # Expected Accuracy Guide
    st.markdown('<p class="section-header" style="margin-top: 2rem;">What Accuracy Should I Expect?</p>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="glass-card">
        <p style="color: #a0aec0; margin-bottom: 1rem;">
            With 77 different intents, random guessing would give you ~1.3% accuracy. 
            Here's what to realistically expect based on dataset size:
        </p>
        <table style="width: 100%; border-collapse: collapse; color: #a0aec0;">
            <thead>
                <tr style="border-bottom: 2px solid #4a5568;">
                    <th style="padding: 0.75rem; text-align: left; color: white;">Dataset Size</th>
                    <th style="padding: 0.75rem; text-align: center; color: white;">Expected Accuracy</th>
                    <th style="padding: 0.75rem; text-align: left; color: white;">Notes</th>
                </tr>
            </thead>
            <tbody>
                <tr style="border-bottom: 1px solid #2d3748;">
                    <td style="padding: 0.75rem;">100 samples</td>
                    <td style="padding: 0.75rem; text-align: center;"><span style="color: #f59e0b;">10-20%</span></td>
                    <td style="padding: 0.75rem;">Limited data, but 10x better than random!</td>
                </tr>
                <tr style="border-bottom: 1px solid #2d3748;">
                    <td style="padding: 0.75rem;">500 samples</td>
                    <td style="padding: 0.75rem; text-align: center;"><span style="color: #f59e0b;">40-50%</span></td>
                    <td style="padding: 0.75rem;">Reasonable for prototypes</td>
                </tr>
                <tr style="border-bottom: 1px solid #2d3748;">
                    <td style="padding: 0.75rem;">1,000 samples</td>
                    <td style="padding: 0.75rem; text-align: center;"><span style="color: #10b981;">60-70%</span></td>
                    <td style="padding: 0.75rem;">Good for initial production</td>
                </tr>
                <tr style="border-bottom: 1px solid #2d3748;">
                    <td style="padding: 0.75rem;">5,000 samples</td>
                    <td style="padding: 0.75rem; text-align: center;"><span style="color: #10b981;">80-85%</span></td>
                    <td style="padding: 0.75rem;">Production-ready</td>
                </tr>
                <tr>
                    <td style="padding: 0.75rem;">10,000+ samples</td>
                    <td style="padding: 0.75rem; text-align: center;"><span style="color: #10b981;">90%+</span></td>
                    <td style="padding: 0.75rem;">State-of-the-art performance</td>
                </tr>
            </tbody>
        </table>
        <p style="color: #718096; font-size: 0.85rem; margin-top: 1rem;">
            üí° Remember: Quality matters more than quantity. 500 high-quality samples can outperform 2000 low-quality ones.
        </p>
    </div>
    """, unsafe_allow_html=True)


def run_training(
    conversations: list,
    model_type: str,
    use_cv: bool,
    cv_folds: int,
    max_features: int,
    ngram_range: tuple,
    test_size: float,
    save_model: bool,
    dataset_path: str = None
):
    """Execute the training process."""
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    if BACKEND_AVAILABLE:
        try:
            status_text.text("Initializing classifier...")
            progress_bar.progress(10)
            
            # Create classifier
            classifier = IntentClassifier(
                model_type=model_type,
                max_features=max_features,
                ngram_range=ngram_range
            )
            
            if use_cv:
                status_text.text(f"Running {cv_folds}-fold cross-validation...")
                progress_bar.progress(30)
                
                # Use Trainer for CV
                trainer = Trainer()
                cv_result = trainer.cross_validate(
                    conversations=conversations,
                    k=cv_folds,
                    model_type=model_type
                )
                
                progress_bar.progress(90)
                
                results = {
                    "accuracy": cv_result.mean_accuracy,
                    "accuracy_std": cv_result.std_accuracy,
                    "f1_score": cv_result.mean_f1,
                    "f1_std": cv_result.std_f1,
                    "cv_folds": cv_folds,
                    "model_type": model_type,
                    "is_cv": True
                }
            else:
                status_text.text("Training classifier...")
                progress_bar.progress(30)
                
                # Regular training
                training_result = classifier.train(
                    conversations=conversations,
                    test_size=test_size
                )
                
                progress_bar.progress(70)
                
                if save_model:
                    status_text.text("Saving model...")
                    model_path = f"models/trained/intent_classifier.pkl"
                    classifier.save(model_path)
                
                progress_bar.progress(90)
                
                results = {
                    "accuracy": training_result.accuracy,
                    "f1_score": training_result.f1_score,
                    "precision": training_result.precision,
                    "recall": training_result.recall,
                    "train_samples": training_result.train_samples,
                    "test_samples": training_result.test_samples,
                    "unique_intents": training_result.unique_intents,
                    "model_type": model_type,
                    "is_cv": False,
                    "model_saved": save_model
                }
            
            progress_bar.progress(100)
            status_text.text("Training complete!")
            
            st.session_state.last_training_results = results
            
            # Register training run if registry available
            if REGISTRY_AVAILABLE and dataset_path:
                try:
                    registry = DatasetRegistry()
                    # Try to find dataset ID by path
                    datasets = registry.list_datasets()
                    for ds in datasets:
                        if ds.get("file_path") == dataset_path or dataset_path.endswith(ds.get("name", "")):
                            registry.register_training_run(
                                dataset_id=ds["id"],
                                model_type=model_type,
                                results=results
                            )
                            break
                except Exception:
                    pass  # Silent fail for registry
            
            st.rerun()
            
        except Exception as e:
            progress_bar.progress(100)
            status_text.text("Training failed")
            st.error(f"Error: {str(e)}")
    else:
        # Demo mode
        import time
        for i in range(5):
            progress_bar.progress((i + 1) * 20)
            status_text.text(f"Simulating training... Step {i + 1}/5")
            time.sleep(0.3)
        
        status_text.text("Demo training complete!")
        
        st.session_state.last_training_results = {
            "accuracy": 0.68,
            "f1_score": 0.62,
            "precision": 0.65,
            "recall": 0.60,
            "train_samples": int(len(conversations) * 0.8),
            "test_samples": int(len(conversations) * 0.2),
            "unique_intents": min(len(conversations), 77),
            "model_type": model_type,
            "is_cv": use_cv,
            "demo": True
        }
        
        if use_cv:
            st.session_state.last_training_results["accuracy_std"] = 0.05
            st.session_state.last_training_results["f1_std"] = 0.06
            st.session_state.last_training_results["cv_folds"] = cv_folds
        
        st.rerun()


def run_evaluation(model_path: str, test_data_path: str):
    """Run model evaluation on test data."""
    try:
        # Load model
        classifier = IntentClassifier.load(model_path)
        
        # Load test data
        conversations = load_conversations(test_data_path)
        
        # Evaluate
        result = classifier.evaluate(conversations)
        
        st.session_state.last_evaluation_results = {
            "accuracy": result.accuracy,
            "f1_score": result.f1_score,
            "precision": result.precision,
            "recall": result.recall,
            "test_samples": len(conversations)
        }
        
        st.rerun()
        
    except Exception as e:
        st.error(f"Evaluation error: {e}")


def show_training_results(results: dict):
    """Display training results."""
    
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    
    if results.get("demo"):
        info_banner("Demo results - connect backend for real training.", type="info")
    
    # Model info
    model_type = results.get("model_type", "unknown").replace("_", " ").title()
    is_cv = results.get("is_cv", False)
    
    st.markdown(f"**Model:** {model_type}")
    if is_cv:
        st.markdown(f"**Cross-Validation:** {results.get('cv_folds', 5)}-fold")
    
    st.markdown("---")
    
    # Metrics
    col1, col2 = st.columns(2)
    
    accuracy = results.get("accuracy", 0)
    f1 = results.get("f1_score", 0)
    
    with col1:
        if is_cv and "accuracy_std" in results:
            value = f"{accuracy*100:.1f}% ¬± {results['accuracy_std']*100:.1f}%"
        else:
            value = f"{accuracy*100:.1f}%"
        
        metric_card(
            title="Accuracy",
            value=value,
            subtitle="Test set performance",
            status="success" if accuracy >= 0.7 else "warning" if accuracy >= 0.5 else "error",
            icon="üéØ"
        )
    
    with col2:
        if is_cv and "f1_std" in results:
            value = f"{f1*100:.1f}% ¬± {results['f1_std']*100:.1f}%"
        else:
            value = f"{f1*100:.1f}%"
        
        metric_card(
            title="F1 Score",
            value=value,
            subtitle="Macro average",
            status="success" if f1 >= 0.65 else "warning" if f1 >= 0.45 else "error",
            icon="üìä"
        )
    
    if not is_cv:
        col3, col4 = st.columns(2)
        
        with col3:
            precision = results.get("precision", 0)
            metric_card(
                title="Precision",
                value=f"{precision*100:.1f}%",
                subtitle="Positive predictions",
                status="neutral",
                icon="‚úì"
            )
        
        with col4:
            recall = results.get("recall", 0)
            metric_card(
                title="Recall",
                value=f"{recall*100:.1f}%",
                subtitle="True positive rate",
                status="neutral",
                icon="üîç"
            )
    
    st.markdown("---")
    
    # Dataset info
    if "train_samples" in results:
        info_text = f"Trained on {results['train_samples']} samples, tested on {results['test_samples']} samples"
        if "unique_intents" in results:
            info_text += f" ({results['unique_intents']} intents)"
        st.caption(info_text)
    
    if results.get("model_saved"):
        st.success("Model saved to `models/trained/intent_classifier.pkl`")
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # Accuracy gauge
    st.markdown('<div class="glass-card" style="margin-top: 1rem;">', unsafe_allow_html=True)
    fig = quality_gauge(
        accuracy * 100,
        title="Model Accuracy",
        height=250
    )
    st.plotly_chart(fig, use_container_width=True)
    st.markdown('</div>', unsafe_allow_html=True)


def show_evaluation_results(results: dict):
    """Display evaluation results."""
    st.markdown('<p class="section-header" style="margin-top: 2rem;">Evaluation Results</p>', unsafe_allow_html=True)
    
    if results.get("demo"):
        info_banner("Demo results - connect backend for real evaluation.", type="info")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        stat_card(
            value=f"{results.get('accuracy', 0)*100:.1f}%",
            label="Accuracy",
            icon="üéØ"
        )
    
    with col2:
        stat_card(
            value=f"{results.get('f1_score', 0)*100:.1f}%",
            label="F1 Score",
            icon="üìä"
        )
    
    with col3:
        stat_card(
            value=f"{results.get('precision', 0)*100:.1f}%",
            label="Precision",
            icon="‚úì"
        )
    
    with col4:
        stat_card(
            value=f"{results.get('recall', 0)*100:.1f}%",
            label="Recall",
            icon="üîç"
        )


def _get_preset_model_type(preset_name: str) -> str:
    """Get the model type for a given preset."""
    preset_models = {
        "fast": "logistic_regression",
        "balanced": "random_forest",
        "best": "xgboost",
        "quick_test": "logistic_regression",
        "cross_validation": "random_forest"
    }
    return preset_models.get(preset_name, "logistic_regression")
